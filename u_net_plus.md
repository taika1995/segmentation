# U-Net++阅读笔记
## 一些铺垫
### FCN(全卷积网络) 

### U-Net
>**1. 华点提炼**  
>+ 下采样  
>+ 上采样  
>+ skip connection   

>**2. U-Net网络结构图**
![](../../../Documents/paper/learning/figs/unet_structure.png)  
如图所示，U-Net的第一个特点是完全对称；skip connection使用叠操作（concatenation）；结构是一个典型的编码解码（encoder-decoder）思路。  
输入一幅图，经过编码（降采样）、解码（升采样），输出目标的分割结果。根据结果和真是分割的差异，反向传播来训练该分割网络，以达到尽可能缩小差异的效果。  



## 引出的问题
`该拓扑结构要多深合适？`--->`降采样对分割网络到底是不是必须的？`
### 回答问题一：几个网络结构对比
>![](https://pic1.zhimg.com/v2-b36ee2a29c817fc3d654577d4eb503f0_r.jpg)  
>![](https://pic2.zhimg.com/80/v2-f360fa394048327274b9eecd0d6342b9_720w.jpg)
>+ 该结构不是越深越好，所以类似的细节参数调整并不会改善本质结构以及提升科研水平。。。
>+ 类似bottleneck、residual以及densenet等对encoder上的微创络绎不绝，但上述优化会不自觉地局限自己的拓展空间。  

浅层结构可以抓取图像简单特征，而深层结构因为感受野变大，经过卷积操作增加，能抓取一些“说不清楚”的抽象特征。  
### 回答问题二：降采样的意义
>+ 可增加对输入图像一些扰动的鲁棒性：平移、旋转；
>+ 减少过拟合风险；
>+ 降低运算量；
>+ 增加感受野；  

升采样的最大作用就是把抽象特征还原到原图尺寸，得到分割结果。  
所以`为什么U-Net要降到第四层才开始上采样回去？`  
### U-Net不同深度对比实验结果
>![](https://pic1.zhimg.com/80/v2-b70bb7e451954a0c88accaf5da36f2d4_720w.jpg)  
> 从不同深度的U-Net表现可发现，不同层次特征的重要性对不同的数据集是不同的，所以如何**融合浅层和深层特征**是关键。



## U-Net++主体演变  
> ![](https://pic4.zhimg.com/80/v2-8b76a55017c4cb60270880d9ac58b1a3_720w.jpg)  
> 结合不同深度特征；共享一个特征提取器。但是该网络无法被训练:  
> ![](https://pic1.zhimg.com/80/v2-672c0f585a2bc78098af79bd0f82f438_720w.jpg)  
> 红色区域在loss function反向传播时是断开的，如何解决？  
> ![](https://pic1.zhimg.com/80/v2-9bee627e3ff09bd89ae4cb4e31231d2c_720w.jpg)  
> 修改结构如上图--->“Deep Layer Aggregation”  
> 该结构强行去掉了U-Net本身自带的长连接，长连接联系输入图像的很多信息，有助于还原降采样所带来的信息损失，所以引出综合长连接和短连接的方案--->“U-Net++”：  
> ![](https://pic1.zhimg.com/80/v2-36e3f4c3342bf872fd5fcb8186f91c5c_720w.jpg)  

又出现问题了：`这个网络比U-Net效果好，但是这个网络增加了多少的参数，加粗的参数可都是比U-Net多出来的啊？`  
> ![](https://pic3.zhimg.com/80/v2-3a3ec245ec8f0296c2b29fa581a0a93e_720w.jpg)  
> ![](https://pic3.zhimg.com/80/v2-5185da01244a94599771877d2dad7612_720w.jpg)
论文实验表明，单纯把网络变宽，把参数提上去对效果提升不大，如何把参数用在刀刃上很重要。



## U-Net++另一亮点
针对`如果只用最右侧一个loss，该结构在反向传播过程中中间结构接收不到过来的梯度`问题，一个直接的解决方案是**深监督**，且该结构可以被**剪枝**：  
> ![](https://pic2.zhimg.com/80/v2-debfd1acf4b9f2a63eea5db0fe920ef5_720w.jpg)  
>+ `为什么可以剪枝` 
>+ `如何剪枝`  
>+ `好处在哪里`

>![](https://pic1.zhimg.com/80/v2-53e23bbec3f7667ad87a2513667b1c08_720w.jpg)  
> 在测试阶段，由于输入图像只会向前传播，扔掉这部分也不会对前面的输出有影响；
> 在训练阶段，因为既有前向又有反向传播，被剪掉的部分会帮助其他部分做权重更新。
> 所以，如果小的子网络输出结果足够好了，我们可以随意剪掉多余部分。
> 可根据子网络在验证集的结果来决定剪多少。



## U-Net++总结
>+ 精度提升：整合不同层次特征；
>+ 参数量巨大的深度网络在可接受精度范围内大幅度减少参数量：灵活的网络结构配合深监督。
